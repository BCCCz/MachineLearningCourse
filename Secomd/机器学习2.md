## 机器学习第二次上机

**实验名称：无监督学习之K-means聚类、DBSCAN算法实验**

### 一.实验要求

(1)理解K-means聚类算法、DBSCAN算法的基本原理

(2)学会用python实现K-means和DBSCAN算法算法





### 二.实验内容





### 三.实验相关技术介绍

**1.K-means算法原理**

k-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。数据对象间距离的计算有很多种，k-means算法通常采用欧氏距离来计算数据对象间的距离。 

K-MEANS算法是输入聚类个数k，以及包含 n个数据对象的数据库，输出满足方差最小标准k个聚类的一种算法。k-means 算法接受输入量 k ；然后将n个数据对象划分为 k个聚类以便使得所获得的聚类满足：同一聚类中的对象相似度较高；而不同聚类中的对象相似度较小。
聚类相似度是利用各聚类中对象的均值所获得一个“中心对象”（引力中心）来进行计算的。

**K-means算法步骤**

- 从 n个数据对象任意选择 k 个对象作为初始聚类中心
- 根据每个聚类对象的均值（中心对象），计算每个对象与这些中心对象的距离；并根据最小距离重新对相应对象进行划分
- 重新计算每个（有变化）聚类的均值（中心对象）
- 计算标准测度函数，当满足一定条件，如函数收敛时，则算法终止；如果条件不满足则回到第二部

**2.DBSCAN算法原理**

DBSCAN（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。 

这类密度聚类算法一般假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。

**流程：**对每个数据点为圆心，以eps为半径画个圈，然后数有多少个点在这个圈内，这个数就是该点密度值。然后我们可以选取一个密度阈值MinPts，如圈内点数小于MinPts的圆心点为低密度的点，而大于或等于MinPts的圆心点高密度的点。如果有一个高密度的点在另一个高密度的点的圈内，我们就把这两个点连接起来，这样我们可以把好多点不断地串联出来。之后，如果有低密度的点也在高密度的点的圈内，把它也连到最近的高密度点上，称之为边界点。这样所有能连到一起的点就成一了个簇，而不在任何高密度点的圈内的低密度点就是异常点。

### 五.过程中相关数据图的记录

#### 1.数据导入

Iris数据集在模式识别研究领域应该是最知名的数据集。这个数据集里一共包括150行记录

**鸢尾花数据集**目的是通过花瓣的长度和宽度，及花萼的长度和宽度，预测出花的品种。

其中前四列为花萼长度，花萼宽度，花瓣长度，花瓣宽度等4个用于识别鸢尾花的属性，第5列为鸢尾花的类别（包括Setosa，Versicolour，Virginica三类）

也即通过判定花萼长度，花萼宽度，花瓣长度，花瓣宽度的尺寸大小来识别鸢尾花的类别

```python
from sklearn.datasets import load_iris

def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data, result
```

下面将数据集分为训练集和测试集，随机选择训练集和数据集

test_size测试集的大小，一般为float

```python
data_train, data_test, result_train, result_test = train_test_split(data, result, test_size=0.25)
```

#### 2.过程



##### 













