## BC

## 机器学习第一次上机实验

### 实验环境

- Anaconda 5.2
- Win10
- Visual Studio Code

### Anaconda安装

清华大学镜像站：https://mirrors.tuna.tsinghua.edu.cn/anaconda/

### 数据集导入

Iris数据集在模式识别研究领域应该是最知名的数据集。这个数据集里一共包括150行记录

其中前四列为花萼长度，花萼宽度，花瓣长度，花瓣宽度等4个用于识别鸢尾花的属性，第5列为鸢尾花的类别（包括Setosa，Versicolour，Virginica三类）

也即通过判定花萼长度，花萼宽度，花瓣长度，花瓣宽度的尺寸大小来识别鸢尾花的类别

```python
from sklearn.datasets import load_iris

def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data, result
```

下面将数据集分为训练集和测试集，随机选择训练集和数据集

test_size测试集的大小，一般为float

```python
data_train, data_test, result_train, result_test = train_test_split(data, result, test_size=0.25)
```

### 实验过程

#### 1.KNN（K-Nearest Neighbor）算法

k近邻算法（k-nearest neighbor, k-NN）是一种基本分类与回归方法，用一张图来理解

![](https://coolshell.cn/wp-content/uploads/2012/08/220px-KnnClassification.svg_.png)

图中的有两个类型的样本数据，一类是蓝色的正方形，另一类是红色的三角形。而那个绿色的圆形是我们待分类的数据。

- 如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。

- 如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。

k近邻算法是在训练数据集中找到与该实例**最邻近**的K个实例，这K个实例的多数属于某个类，我们就说预测点属于哪个类，我们可以有以下几种度量方式：

![](https://pic1.zhimg.com/80/v2-60bb382b0d22ec0ce296ed0e024f31bc_720w.png)

调用机器学习库，KNeighborsClassifier方法中含有8个参数：

1. `n_neighbors` : int, optional (default = 5)
   Number of neighbors to use by default for kneighbors queries…
2. `weights` : str or callable, optional (default = ‘uniform’)
   weight function used in prediction. Possible values::
   - ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.
   - ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
   - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.
3. `algorithm` : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
   Algorithm used to compute the nearest neighbors:
   - ‘ball_tree’ will use BallTree
     ‘kd_tree’ will use KDTree
     ‘brute’ will use a brute-force search.
     ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.
     Note: fitting on sparse input will override the setting of this parameter, using brute force.
4. `leaf_size` : int, optional (default = 30)
   Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.
5. `p` : integer, optional (default = 2)
   Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
6. `metric` : string or callable, default ‘minkowski’
   the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics.
7. `metric_params` : dict, optional (default = None)
   Additional keyword arguments for the metric function.
8. `n_jobs` : int or None, optional (default=None)
   The number of parallel jobs to run for neighbors search. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. Doesn’t affect fit method.

对模型进行训练，输出Predict和我们的测试集进行对比，我们直接调用KNeighborsClassifier，毕竟Anaconda都已经安装了

```python
KNN = KNeighborsClassifier()
KNN.fit(data_train,result_train)

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')

predict = KNN.predict(data_test)
print(predict)
print(result_test)
print(accuracy_score(result_test,predict))
```

输出预测的准确率，大概在0.95左右

![image-20201213212313990](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201213212313990.png)



完整代码：

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data, result

data,result = get_data()

data_train, data_test, result_train, result_test = train_test_split(data, result, test_size=0.25)

KNN = KNeighborsClassifier()
KNN.fit(data_train,result_train)

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')

predict = KNN.predict(data_test)
print(predict)
print(result_test)
print(accuracy_score(result_test,predict))
```



#### 2.Navie Bayes算法（朴素贝叶斯)

通过scikit-learn官网[API说明](http://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)可以看到，sklearn将贝叶斯的三个常用模型都封装好了，分别是：高斯贝叶斯（Gaussian Naive Bayes）、多项式贝叶斯（Multinomial Naive Bayes）、伯努利贝叶斯（Bernoulli Naive Bayes）。

##### (1)高斯朴素贝叶斯（GaussianNB）

在高斯朴素贝叶斯中，每个特征都是连续的，并且都呈高斯分布。高斯分布又称为正态分布。图画出来以后像一个倒挂的钟，以均值为轴对称，如下图所示：

![](https://img-blog.csdn.net/20180710154440529)

GaussianNB 实现了运用于分类的高斯朴素贝叶斯算法。特征的可能性(即概率)假设为高斯分布

![](https://img-blog.csdn.net/20180710153542458)

```python
from sklearn.datasets import load_iris
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data,result

data,result = get_data()
data_train,data_test,result_train,result_test = train_test_split(data,result, test_size=0.3)

Gauss = GaussianNB()
Gauss = Gauss.fit(data_train,result_train)
pred = Gauss.predict(data_test)

print(pred)
print(accuracy_score(result_test,pred))
```

![image-20201214112827707](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201214112827707.png)

正确率基本在90%以上



##### (2)多项分布朴素贝叶斯（MultinomialNB）

##### [`MultinomialNB`](http://scikit-learn.org/dev/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)实现服从多项分布数据（multinomially）的贝叶斯算法，是一个经典的朴素贝叶斯文本分类中使用的变种（其中的数据是通常表示为词向量的数量，虽然TF-IDF向量在实际项目中表现得很好），对于每一个y来说，分布通过向量![img](https://img-blog.csdn.net/20160426211259023?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)参数化，n是类别的数目（在文本分类中，表示词汇量的长度）![img](https://img-blog.csdn.net/20160426212655358?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 表示标签i出现的样本属于类别y的概率![img](https://img-blog.csdn.net/20160426212801641?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)。

##### 参数 ![img](https://img-blog.csdn.net/20160426212655358?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 是一个平滑的最大似然估计，即相对频率计数：

![img](https://img-blog.csdn.net/20180710154832397)

```python
from sklearn.datasets import load_iris
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data,result

data,result = get_data()
data_train,data_test,result_train,result_test = train_test_split(data,result, test_size=0.3)

Multinomial = MultinomialNB()
Multinomial = Multinomial.fit(data_train,result_train)
pred = Multinomial.predict(data_test)

print(pred)
print(accuracy_score(result_test,pred))
```



![image-20201214113145138](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201214113145138.png)

##### (3)伯努利朴素贝叶斯（BernoulliNB）

BernoulliNB 实现了用于多重伯努利分布数据的朴素贝叶斯训练和分类算法，即有多个特征，但每个特征 都假设是一个二元 (Bernoulli, boolean) 变量。 因此，这类算法要求样本以二元值特征向量表示；如果样本含有其他类型的数据， 一个 BernoulliNB 实例会将其二值化(取决于 binarize 参数)。

伯努利朴素贝叶斯的决策规则基于：

![](https://img-blog.csdn.net/20180710155603960)

```python
from sklearn.datasets import load_iris
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data,result

data,result = get_data()
data_train,data_test,result_train,result_test = train_test_split(data,result, test_size=0.3)

Bernoulli = BernoulliNB()
Bernoulli = Bernoulli.fit(data_train,result_train)
pred = Bernoulli.predict(data_test)

print(pred)
print(accuracy_score(result_test,pred))
```



![image-20201214112920700](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201214112920700.png)



正确率较低

#### 3.决策树算法





![image-20201214113416137](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201214113416137.png)