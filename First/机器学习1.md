## 机器学习第一次上机实验

### 一.实验名称

监督学习之KNN、Naive Bayes、决策树算法实验



### 二.实验目的

(1)掌握KNN及Naive Bayes、决策树的原理

(2)学会利用KNN与Navie Bayes解决分类问题

(3)熟练掌握决策树的生成方法与过程



### 三.实验工具

- Anaconda 5.2
- Win10
- Visual Studio Code

Anaconda安装

清华大学镜像站：https://mirrors.tuna.tsinghua.edu.cn/anaconda/



### 四.实验原理

(1) KNN（K-Nearest Neighbor）算法原理

存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。

说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。  

(2) Navie Bayes

朴素贝叶斯分类器中最核心的便是贝叶斯准则，在机器学习中，朴素贝叶斯分类器是一个基于贝叶斯定理的比较简单的概率分类器，其中 naive（朴素）是指的对于模型中各个 feature（特征）有强独立性的假设，并未将feature间的相关性纳入考虑中。

朴素贝叶斯分类器一个比较著名的应用是用于对垃圾邮件分类，通常用文字特征来识别垃圾邮件，是文本分类中比较常用的一种方法。朴素贝叶斯分类通过选择 token（通常是邮件中的单词）来得到垃圾邮件和非垃圾邮件间的关联，再通过贝叶斯定理来计算概率从而对邮件进行分类。

(3) 决策树

决策树是一个非参数的监督式学习方法，主要用于分类和回归。算法的目标是通过推断数据特征，学习决策规则从而创建一个预测目标变量的模型。

### 五.实验步骤

#### 1.数据导入

Iris数据集在模式识别研究领域应该是最知名的数据集。这个数据集里一共包括150行记录

**鸢尾花数据集**目的是通过花瓣的长度和宽度，及花萼的长度和宽度，预测出花的品种。

其中前四列为花萼长度，花萼宽度，花瓣长度，花瓣宽度等4个用于识别鸢尾花的属性，第5列为鸢尾花的类别（包括Setosa，Versicolour，Virginica三类）

也即通过判定花萼长度，花萼宽度，花瓣长度，花瓣宽度的尺寸大小来识别鸢尾花的类别

```python
from sklearn.datasets import load_iris

def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data, result
```

下面将数据集分为训练集和测试集，随机选择训练集和数据集

test_size测试集的大小，一般为float

```python
data_train, data_test, result_train, result_test = train_test_split(data, result, test_size=0.25)
```



#### 2.实验过程

**1.KNN（K-Nearest Neighbor）算法**

k近邻算法（k-nearest neighbor, k-NN）是一种基本分类与回归方法，用一张图来理解

![](https://coolshell.cn/wp-content/uploads/2012/08/220px-KnnClassification.svg_.png)

图中的有两个类型的样本数据，一类是蓝色的正方形，另一类是红色的三角形。而那个绿色的圆形是我们待分类的数据。

- 如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。

- 如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。

k近邻算法是在训练数据集中找到与该实例**最邻近**的K个实例，这K个实例的多数属于某个类，我们就说预测点属于哪个类，我们可以有以下几种度量方式：

![](https://pic1.zhimg.com/80/v2-60bb382b0d22ec0ce296ed0e024f31bc_720w.png)

调用机器学习库，KNeighborsClassifier方法中含有8个参数：

1. `n_neighbors` : int, optional (default = 5)
   Number of neighbors to use by default for kneighbors queries…
2. `weights` : str or callable, optional (default = ‘uniform’)
   weight function used in prediction. Possible values::
   - ‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.
   - ‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
   - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.
3. `algorithm` : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
   Algorithm used to compute the nearest neighbors:
   - ‘ball_tree’ will use BallTree
     ‘kd_tree’ will use KDTree
     ‘brute’ will use a brute-force search.
     ‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.
     Note: fitting on sparse input will override the setting of this parameter, using brute force.
4. `leaf_size` : int, optional (default = 30)
   Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.
5. `p` : integer, optional (default = 2)
   Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.
6. `metric` : string or callable, default ‘minkowski’
   the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics.
7. `metric_params` : dict, optional (default = None)
   Additional keyword arguments for the metric function.
8. `n_jobs` : int or None, optional (default=None)
   The number of parallel jobs to run for neighbors search. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. Doesn’t affect fit method.

输出Predict和我们的测试集进行对比，我们直接调用KNeighborsClassifier

创建KNN模型并进行训练，对测试集进行测试

```python
KNN = KNeighborsClassifier()
KNN.fit(data_train,result_train)

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')

predict = KNN.predict(data_test)

```



**2.Navie Bayes算法（朴素贝叶斯)**

通过scikit-learn官网[API说明](http://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)可以看到，sklearn将贝叶斯的三个常用模型都封装好了，分别是：高斯贝叶斯（Gaussian Naive Bayes）、多项式贝叶斯（Multinomial Naive Bayes）、伯努利贝叶斯（Bernoulli Naive Bayes）。

##### (1)高斯朴素贝叶斯（GaussianNB）

在高斯朴素贝叶斯中，每个特征都是连续的，并且都呈高斯分布。高斯分布又称为正态分布。图画出来以后像一个倒挂的钟，以均值为轴对称，如下图所示：

![](https://img-blog.csdn.net/20180710154440529)

GaussianNB 实现了运用于分类的高斯朴素贝叶斯算法。特征的可能性(即概率)假设为高斯分布

![](https://img-blog.csdn.net/20180710153542458)

创建模型:

```python
Gauss = GaussianNB()
Gauss = Gauss.fit(data_train,result_train)
pred = Gauss.predict(data_test)
```



##### (2)多项分布朴素贝叶斯（MultinomialNB）

##### [`MultinomialNB`](http://scikit-learn.org/dev/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)实现服从多项分布数据（multinomially）的贝叶斯算法，是一个经典的朴素贝叶斯文本分类中使用的变种（其中的数据是通常表示为词向量的数量，虽然TF-IDF向量在实际项目中表现得很好），对于每一个y来说，分布通过向量![img](https://img-blog.csdn.net/20160426211259023)参数化，n是类别的数目（在文本分类中，表示词汇量的长度）![img](https://img-blog.csdn.net/20160426212655358?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 表示标签i出现的样本属于类别y的概率![img](https://img-blog.csdn.net/20160426212801641)。

##### 参数 ![img](https://img-blog.csdn.net/20160426212655358?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 是一个平滑的最大似然估计，即相对频率计数：

![img](https://img-blog.csdn.net/20180710154832397)

创建模型:

```python
Multinomial = MultinomialNB()
Multinomial = Multinomial.fit(data_train,result_train)
pred = Multinomial.predict(data_test)
```



##### (3)伯努利朴素贝叶斯（BernoulliNB）

BernoulliNB 实现了用于多重伯努利分布数据的朴素贝叶斯训练和分类算法，即有多个特征，但每个特征 都假设是一个二元 (Bernoulli, boolean) 变量。 因此，这类算法要求样本以二元值特征向量表示；如果样本含有其他类型的数据， 一个 BernoulliNB 实例会将其二值化(取决于 binarize 参数)。

伯努利朴素贝叶斯的决策规则基于：

![](https://img-blog.csdn.net/20180710155603960)

创建模型:

```python
Bernoulli = BernoulliNB()
Bernoulli = Bernoulli.fit(data_train,result_train)
pred = Bernoulli.predict(data_test)
```



**3.决策树算法**

决策树是一个包含根节点、若干内部节点和若干叶子节点的树形结构。决策树的根节点包含样本全集，内部节点对应特征属性，叶子节点表示决策的结果。

使用决策树进行判断时，从根节点开始，测试待分类数据的特征属性值，应该走哪条分支，循环这样判断，直到叶子节点为止。最终到达的这个叶子节点，就是决策树的最终决策结果。

决策树模型的学习过程一般有三个阶段：

- **特征选择**：选择哪些属性作为树的节点。
- **生成决策树**：生成树形结构。
- **决策树剪枝**：是决策树的一种优化手段，比如剪去一些不必要的属性节点。一般有“预剪枝”和“后剪枝”两种。
  - 剪枝的**目的**是防止过拟合现象，提高泛化能力。
  - **预剪枝**是在决策树的生成过程中就进行剪枝，缺点是有可能造成欠拟合。
  - **后剪枝**是在决策树生成之后再进行剪枝，缺点是计算量较大。

![](https://img-blog.csdnimg.cn/20201111102818315.png)

创建模型:

```python
Tree = DecisionTreeClassifier()
Tree.fit(data_train,result_train)
pred = Tree.predict(data_test)
```



### 六.实验数据及结果出来

**1.KNN**

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data, result

data,result = get_data()

data_train, data_test, result_train, result_test = train_test_split(data, result, test_size=0.25)

KNN = KNeighborsClassifier()
KNN.fit(data_train,result_train)

KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')

predict = KNN.predict(data_test)
print(predict)
print(result_test)
print(accuracy_score(result_test,predict))
```

![image-20201213212313990](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201213212313990.png)

**2.朴素贝叶斯**

(1)高斯朴素贝叶斯（GaussianNB）

```python
from sklearn.datasets import load_iris
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data,result

data,result = get_data()
data_train,data_test,result_train,result_test = train_test_split(data,result, test_size=0.3)

Gauss = GaussianNB()
Gauss = Gauss.fit(data_train,result_train)
pred = Gauss.predict(data_test)

print(pred)
print(accuracy_score(result_test,pred))
```

![image-20201214112827707](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201214112827707.png)

正确率基本在90%以上

(2)多项分布朴素贝叶斯（MultinomialNB）

```python
from sklearn.datasets import load_iris
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data,result

data,result = get_data()
data_train,data_test,result_train,result_test = train_test_split(data,result, test_size=0.3)

Multinomial = MultinomialNB()
Multinomial = Multinomial.fit(data_train,result_train)
pred = Multinomial.predict(data_test)

print(pred)
print(accuracy_score(result_test,pred))
```



![image-20201214113145138](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201214113145138.png)

(3)伯努利朴素贝叶斯（BernoulliNB）

```python
from sklearn.datasets import load_iris
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split


def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data,result

data,result = get_data()
data_train,data_test,result_train,result_test = train_test_split(data,result, test_size=0.3)

Bernoulli = BernoulliNB()
Bernoulli = Bernoulli.fit(data_train,result_train)
pred = Bernoulli.predict(data_test)

print(pred
print(accuracy_score(result_test,pred))
```

![image-20201214112920700](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201214112920700.png)

正确率较低

**3.决策树**

```python
from sklearn.tree import DecisionTreeClassifier  
from sklearn.datasets import load_iris  
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import accuracy_score

def get_data():
    iris = load_iris()
    data = iris.data
    result = iris.target
    return data,result

data,result = get_data()
data_train,data_test,result_train,result_test = train_test_split(data,result, test_size=0.3)
Tree = DecisionTreeClassifier()
Tree.fit(data_train,result_train)
pred = Tree.predict(data_test)

print(pred)
print(result_test)
print(accuracy_score(result_test,pred))
```

预测结果：

![image-20201214113416137](C:\Users\BC\AppData\Roaming\Typora\typora-user-images\image-20201214113416137.png)

为了清楚的知道，我们构造出的这个决策树 到底是什么样子，我们将决策树画出来

### 七.实验结论

#### 算法优缺点比较

##### 1.KNN

**优点**

- 简单，易于理解，易于实现，无需估计参数，无需训练的方法

**缺点**

- 计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离

##### 2.朴素贝叶斯

**优点**

- 稳定的分类效率。对缺失数据不太敏感，算法也比较简单

**缺点**

- 某些时候会由于假设的先验模型的原因导致预测效果不佳

##### 3.决策树

**优点**

- 计算复杂度不高
- 输出结果易于理解。

**缺点**

-  可能产生过拟合问题，在我测试时候出选了所有测试样本全部正确的情况

#### 收获

熟悉了调用机器学习库函数，了解到了三种机器学习算法基本原理